<!DOCTYPE html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><meta name="theme" content="Chirpy v2.6.2"><meta name="generator" content="Jekyll v4.1.1" /><meta property="og:title" content="A k-armed Bandit Problem" /><meta name="author" content="Bekay" /><meta property="og:locale" content="en_US" /><meta name="description" content="A k-armed Bandit Problem k-armed bandit problem은 아래와 같다. Consider the following learning problem. You are faced repeatedly with a choice among k different options, or actions, After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period. Expected total reward를 최대화하는 것이 bandit problem의 목표이다. 어떤 Action에는 해당하는 expected or mean reward가 있다(Expected value는 구하기 어렵기 때문에 Mean value로 흔히 근사한다). Expected or Mean reward를 우리는 Value of the action이라 부른다." /><meta property="og:description" content="A k-armed Bandit Problem k-armed bandit problem은 아래와 같다. Consider the following learning problem. You are faced repeatedly with a choice among k different options, or actions, After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period. Expected total reward를 최대화하는 것이 bandit problem의 목표이다. 어떤 Action에는 해당하는 expected or mean reward가 있다(Expected value는 구하기 어렵기 때문에 Mean value로 흔히 근사한다). Expected or Mean reward를 우리는 Value of the action이라 부른다." /><link rel="canonical" href="https://bekaykang.github.io/posts/RL-2-1/" /><meta property="og:url" content="https://bekaykang.github.io/posts/RL-2-1/" /><meta property="og:site_name" content="Bekay" /><meta property="og:image" content="https://bekaykang.github.io/assets/img/post/201231-1.png" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2020-12-31T09:25:00+09:00" /><meta name="twitter:card" content="summary_large_image" /><meta property="twitter:image" content="https://bekaykang.github.io/assets/img/post/201231-1.png" /><meta property="twitter:title" content="A k-armed Bandit Problem" /><meta name="twitter:site" content="@twitter_username" /><meta name="twitter:creator" content="@Bekay" /><meta name="google-site-verification" content="google447ef08cce8dcf1c.html" /> <script type="application/ld+json"> {"author":{"@type":"Person","name":"Bekay"},"headline":"A k-armed Bandit Problem","dateModified":"2021-05-16T15:52:21+09:00","datePublished":"2020-12-31T09:25:00+09:00","description":"A k-armed Bandit Problem k-armed bandit problem은 아래와 같다. Consider the following learning problem. You are faced repeatedly with a choice among k different options, or actions, After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period. Expected total reward를 최대화하는 것이 bandit problem의 목표이다. 어떤 Action에는 해당하는 expected or mean reward가 있다(Expected value는 구하기 어렵기 때문에 Mean value로 흔히 근사한다). Expected or Mean reward를 우리는 Value of the action이라 부른다.","url":"https://bekaykang.github.io/posts/RL-2-1/","mainEntityOfPage":{"@type":"WebPage","@id":"https://bekaykang.github.io/posts/RL-2-1/"},"@type":"BlogPosting","image":"https://bekaykang.github.io/assets/img/post/201231-1.png","@context":"https://schema.org"}</script><title>A k-armed Bandit Problem | Bekay</title><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="icon" href="/assets/img/favicons/favicon.ico" type="image/x-icon"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon.png"><link rel="apple-touch-icon" href="/assets/img/favicons/apple-icon-precomposed.png"><link rel="apple-touch-icon" sizes="57x57" href="/assets/img/favicons/apple-icon-57x57.png"><link rel="apple-touch-icon" sizes="60x60" href="/assets/img/favicons/apple-icon-60x60.png"><link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicons/apple-icon-72x72.png"><link rel="apple-touch-icon" sizes="76x76" href="/assets/img/favicons/apple-icon-76x76.png"><link rel="apple-touch-icon" sizes="114x114" href="/assets/img/favicons/apple-icon-114x114.png"><link rel="apple-touch-icon" sizes="120x120" href="/assets/img/favicons/apple-icon-120x120.png"><link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicons/apple-icon-144x144.png"><link rel="apple-touch-icon" sizes="152x152" href="/assets/img/favicons/apple-icon-152x152.png"><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-icon-180x180.png"><link rel="icon" type="image/png" sizes="192x192" href="/assets/img/favicons/android-icon-192x192.png"><link rel="icon" type="image/png" sizes="32x32" href="/assets/img/favicons/favicon-32x32.png"><link rel="icon" type="image/png" sizes="96x96" href="/assets/img/favicons/favicon-96x96.png"><link rel="icon" type="image/png" sizes="16x16" href="/assets/img/favicons/favicon-16x16.png"><link rel="manifest" href="/assets/img/favicons/manifest.json"><meta name='msapplication-config' content='/assets/img/favicons/browserconfig.xml'><meta name="msapplication-TileColor" content="#ffffff"><meta name="msapplication-TileImage" content="/assets/img/favicons/ms-icon-144x144.png"><meta name="theme-color" content="#ffffff"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://fonts.gstatic.com"><link rel="preconnect" href="https://www.google-analytics.com" crossorigin="use-credentials"><link rel="dns-prefetch" href="https://www.google-analytics.com"><link rel="preconnect" href="https://www.googletagmanager.com" crossorigin="anonymous"><link rel="dns-prefetch" href="https://www.googletagmanager.com"><link rel="preconnect" href="cdn.jsdelivr.net"><link rel="dns-prefetch" href="cdn.jsdelivr.net"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.0.0/dist/css/bootstrap.min.css" integrity="sha256-LA89z+k9fjgMKQ/kq4OO2Mrf8VltYml/VES+Rg0fh20=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous"><link rel="preload" href="/assets/css/post.css" as="style"><link rel="stylesheet" href="/assets/css/post.css"><link rel="preload" as="style" href="/assets/css/lib/bootstrap-toc.min.css"><link rel="stylesheet" href="/assets/css/lib/bootstrap-toc.min.css" /> <script src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script> <script defer src="https://cdn.jsdelivr.net/combine/npm/popper.js@1.15.0,npm/bootstrap@4/dist/js/bootstrap.min.js"></script> <script async src="/assets/js/post.min.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script> <script defer src="/app.js"></script> <script defer src="https://www.googletagmanager.com/gtag/js?id=G-14G0S6F33W"></script> <script> document.addEventListener("DOMContentLoaded", function(event) { window.dataLayer = window.dataLayer || []; function gtag(){dataLayer.push(arguments);} gtag('js', new Date()); gtag('config', 'G-14G0S6F33W'); }); </script><body data-spy="scroll" data-target="#toc"><div id="sidebar" class="d-flex flex-column"><div id="nav-wrapper"><div id="profile-wrapper" class="d-flex flex-column"><div id="avatar" class="d-flex justify-content-center"> <a href="/" alt="avatar"> <img src="/assets/img/sample/avatar_bekay.jpg" alt="avatar" onerror="this.style.display='none'"> </a></div><div class="profile-text mt-3"><div class="site-title"> <a href="/">Bekay</a></div><div class="site-subtitle font-italic">Hello :)</div></div></div><ul class="nav flex-column"><li class="nav-item d-flex justify-content-center "> <a href="/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-home ml-xl-3 mr-xl-3 unloaded"></i> <span>HOME</span> </a><li class="nav-item d-flex justify-content-center "> <a href="/tabs/categories/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-stream ml-xl-3 mr-xl-3 unloaded"></i> <span>CATEGORIES</span> </a><li class="nav-item d-flex justify-content-center "> <a href="/tabs/tags/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-tags ml-xl-3 mr-xl-3 unloaded"></i> <span>TAGS</span> </a><li class="nav-item d-flex justify-content-center "> <a href="/tabs/archives/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-archive ml-xl-3 mr-xl-3 unloaded"></i> <span>ARCHIVES</span> </a><li class="nav-item d-flex justify-content-center "> <a href="/tabs/about/" class="nav-link d-flex justify-content-center align-items-center w-100"> <i class="fa-fw fas fa-info ml-xl-3 mr-xl-3 unloaded"></i> <span>ABOUT</span> </a></ul></div><div class="sidebar-bottom d-flex flex-wrap justify-content-around mt-4"> <span id="mode-toggle-wrapper"> <i class="mode-toggle fas fa-adjust"></i> <script type="text/javascript"> class ModeToggle { static get MODE_KEY() { return "mode"; } static get DARK_MODE() { return "dark"; } static get LIGHT_MODE() { return "light"; } constructor() { if (this.hasMode) { if (this.isDarkMode) { if (!this.isSysDarkPrefer) { this.setDark(); } } else { if (this.isSysDarkPrefer) { this.setLight(); } } } var self = this; /* always follow the system prefers */ this.sysDarkPrefers.addListener(function() { if (self.hasMode) { if (self.isDarkMode) { if (!self.isSysDarkPrefer) { self.setDark(); } } else { if (self.isSysDarkPrefer) { self.setLight(); } } self.clearMode(); } self.updateMermaid(); }); } /* constructor() */ setDark() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.DARK_MODE); } setLight() { $('html').attr(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); sessionStorage.setItem(ModeToggle.MODE_KEY, ModeToggle.LIGHT_MODE); } clearMode() { $('html').removeAttr(ModeToggle.MODE_KEY); sessionStorage.removeItem(ModeToggle.MODE_KEY); } get sysDarkPrefers() { return window.matchMedia("(prefers-color-scheme: dark)"); } get isSysDarkPrefer() { return this.sysDarkPrefers.matches; } get isDarkMode() { return this.mode == ModeToggle.DARK_MODE; } get isLightMode() { return this.mode == ModeToggle.LIGHT_MODE; } get hasMode() { return this.mode != null; } get mode() { return sessionStorage.getItem(ModeToggle.MODE_KEY); } /* get the current mode on screen */ get modeStatus() { if (this.isDarkMode || (!this.hasMode && this.isSysDarkPrefer) ) { return ModeToggle.DARK_MODE; } else { return ModeToggle.LIGHT_MODE; } } updateMermaid() { if (typeof mermaid !== "undefined") { let expectedTheme = (this.modeStatus === ModeToggle.DARK_MODE? "dark" : "default"); let config = { theme: expectedTheme }; /* re-render the SVG › <https://github.com/mermaid-js/mermaid/issues/311#issuecomment-332557344> */ $(".mermaid").each(function() { let svgCode = $(this).prev().children().html(); $(this).removeAttr("data-processed"); $(this).html(svgCode); }); mermaid.initialize(config); mermaid.init(undefined, ".mermaid"); } } flipMode() { if (this.hasMode) { if (this.isSysDarkPrefer) { if (this.isLightMode) { this.clearMode(); } else { this.setLight(); } } else { if (this.isDarkMode) { this.clearMode(); } else { this.setDark(); } } } else { if (this.isSysDarkPrefer) { this.setLight(); } else { this.setDark(); } } this.updateMermaid(); } /* flipMode() */ } /* ModeToggle */ let toggle = new ModeToggle(); $(".mode-toggle").click(function() { toggle.flipMode(); }); </script> </span> <span class="icon-border"></span> <a href="https://github.com/BekayKang" aria-label="github" target="_blank" rel="noopener"> <i class="fab fa-github-alt"></i> </a> <a href="https://twitter.com/twitter_username" aria-label="twitter" target="_blank" rel="noopener"> <i class="fab fa-twitter"></i> </a> <a href=" javascript:location.href = 'mailto:' + ['bekay.kang','gmail.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div><div class="hits mt-2"> <center> <a href="https://hits.seeyoufarm.com"><img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fbekaykang.github.io&count_bg=%2379C83D&title_bg=%23F50505&icon=ulule.svg&icon_color=%23E7E7E7&title=Visitors&edge_flat=false"/></a> </center></div></div><div id="topbar-wrapper" class="row justify-content-center topbar-down"><div id="topbar" class="col-11 d-flex h-100 align-items-center justify-content-between"> <span id="breadcrumb"> <span> <a href="/"> Posts </a> </span> <span>A k-armed Bandit Problem</span> </span> <i id="sidebar-trigger" class="fas fa-bars fa-fw"></i><div id="topbar-title"> Post</div><i id="search-trigger" class="fas fa-search fa-fw"></i> <span id="search-wrapper" class="align-items-center"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" placeholder="Search..."> <i class="fa fa-times-circle fa-fw" id="search-cleaner"></i> </span> <span id="search-cancel" >Cancel</span></div></div><div id="main-wrapper"><div id="main"><div class="row"><div id="post-wrapper" class="col-12 col-lg-11 col-xl-8"><div class="post pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><h1 data-toc-skip>A k-armed Bandit Problem</h1><div class="post-meta text-muted d-flex flex-column"><div> Posted <span class="timeago " data-toggle="tooltip" data-placement="bottom" title="Thu, Dec 31, 2020, 9:25 AM +0900" > Dec 31, 2020 <i class="unloaded">2020-12-31T09:25:00+09:00</i> </span> by <span class="author"> Bekay </span></div><div> Updated <span class="timeago lastmod" data-toggle="tooltip" data-placement="bottom" title="Sun, May 16, 2021, 3:52 PM +0900" > May 16, 2021 <i class="unloaded">2021-05-16T15:52:21+09:00</i> </span></div></div><div class="post-content"> <img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7 " data-src="/assets/img/post/201231-1.png" class="post-preview-img"><hr /><h2 id="a-k-armed-bandit-problem">A k-armed Bandit Problem</h2><hr /><p>k-armed bandit problem은 아래와 같다.</p><blockquote><p>Consider the following learning problem. You are faced repeatedly with a choice among k different options, or actions, After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. <strong>Your objective is to maximize the expected total reward over some time period.</strong></p></blockquote><p><strong>Expected total reward</strong>를 최대화하는 것이 bandit problem의 목표이다. 어떤 Action에는 해당하는 expected or mean reward가 있다(Expected value는 구하기 어렵기 때문에 Mean value로 흔히 근사한다). <strong>Expected or Mean reward</strong>를 우리는 <strong>Value of the action</strong>이라 부른다.</p><blockquote><p>We denote the action selected on time step t as \(A_t\), and the corresponding reward as \(R_t\). The value then of an arbitrary actiona, denoted \(q_*(a)\), is the expected reward given that a is selected: <br /> <strong>\(q_*(a)\doteq\mathbb{E}[R_t|A_t=a]\)</strong></p></blockquote><p><strong>하지만 대부분의 풀어야하는 문제에서는 우리는 value of the action을 알지 못 한다. 따라서 우리는 estimation을 통해서 value of the action을 유추하고자 한다.</strong> 그리고 그 추정 값(estimated value)를 \(Q_t(a)\)로 denote한다. 결국 \(Q_t(a)\)가 최대한 \(q_*(a)\)와 같도록 하면 된다.</p><p>만약, \(Q_t(a)\)가 \(q_*(a)\)와 정확하게 같다면 우리는 Multi-armed Bandit 문제를 쉽게 풀 수 있다. \(Q_t(a)\)가 최대가 되는 action을 취하면 우리가 목표로하는 Expected total reward를 최대화 할 수 있다.</p><hr /><h2 id="exploration-and-exploitation">Exploration and Exploitation</h2><hr /><p>위에서 말한 \(Q_t(a)\)의 최대 값인 action을 <strong>greedy actions</strong>라 부른다. 그렇다면 greedy action만 선택하면 최선일까? 그렇지 않은 경우가 대부분이다. 왜냐하면 앞서 말한 것 처럼 우리는 value of the action인 \(q_*(a)\)를 모르기 때문에 estimated value of the action인 \(Q_t(a)\)를 통해 그저 유추할 뿐이다.</p><p><strong>\(Q_t(a)\)의 greedy action을 취하는 것을 exploitation</strong>이라고 하고, 이는 그 상태에서는 expected reward를 최대화하는 방법이다. <strong>greedy action을 취하지 않고 randomly nongreedy action을 취하는 것을 exploration</strong>이라고 하고, 이는 장기적으로 봤을 때 더 큰 total reward를 획득 할 수 도 있게끔 한다.</p><blockquote><p>When you select one of these actions, we say that you are <strong>exploiting</strong> your current knowledge of the value of the actions. If instead you select one of the nongreedy actions, then we say you are <strong>exploring</strong>, because this enables you to improve your estimate of the nongreedy action’s value. <strong>Exploitation is the right thing to do to maximize the expected reward on the one step, but exploration may produce the greater total reward in the long run.</strong></p></blockquote><p>이렇게 생각해보면 매 순간 exploiting과 exploring을 함께 할 수 있는 action을 취하면 가장 좋겠지만, exploitation과 exploration은 <strong>conflict</strong> 관계이기 때문에 그렇게 할 수 없다.</p><p><strong>Reinforcement learning에서 exploration과 exploitation의 balancing 문제는 중요한 연구분야이다.</strong></p></div><div class="post-tail-wrapper text-muted"><div class="post-meta mb-3"> <i class="far fa-folder-open fa-fw mr-1"></i> <a href='/categories/reinforcement-learning/'>Reinforcement Learning</a>, <a href='/categories/rl-by-sutton-barto/'>RL by Sutton & Barto</a></div><div class="post-tags"> <i class="fa fa-tags fa-fw mr-1"></i> <a href="/tags/reinforcement-learning/" class="post-tag no-text-decoration" >Reinforcement Learning</a> <a href="/tags/rl/" class="post-tag no-text-decoration" >RL</a> <a href="/tags/multi-armed-bandits/" class="post-tag no-text-decoration" >Multi-armed Bandits</a></div><div class="post-tail-bottom d-flex justify-content-between align-items-center mt-3 pt-5 pb-2"><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/">CC BY 4.0</a> by the author.</div><div class="share-wrapper"> <span class="share-label text-muted mr-1">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=A k-armed Bandit Problem - Bekay&url=https://bekaykang.github.io/posts/RL-2-1/" data-toggle="tooltip" data-placement="top" title="Twitter" target="_blank" rel="noopener" aria-label="Twitter"> <i class="fa-fw fab fa-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=A k-armed Bandit Problem - Bekay&u=https://bekaykang.github.io/posts/RL-2-1/" data-toggle="tooltip" data-placement="top" title="Facebook" target="_blank" rel="noopener" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://telegram.me/share?text=A k-armed Bandit Problem - Bekay&url=https://bekaykang.github.io/posts/RL-2-1/" data-toggle="tooltip" data-placement="top" title="Telegram" target="_blank" rel="noopener" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <i class="fa-fw fas fa-link small" onclick="copyLink()" data-toggle="tooltip" data-placement="top" title="Copy link"></i> </span></div></div></div></div></div><div id="panel-wrapper" class="col-xl-3 pl-2 text-muted topbar-down"><div class="access"><div id="access-lastmod" class="post"> <span>Recent Update</span><ul class="post-content pl-0 pb-1 ml-1 mt-2"><li><a href="/posts/huber-loss/">Huber Loss & F.smooth-l1-loss()</a><li><a href="/posts/FDM/">Force Directed Method</a><li><a href="/posts/E169/">E169</a><li><a href="/posts/markdown-latex/">Markdown LaTex Math Symbols</a><li><a href="/posts/KL_divergence/">KL Divergence</a></ul></div><div id="access-tags"> <span>Trending Tags</span><div class="d-flex flex-wrap mt-3 mb-1 mr-3"> <a class="post-tag" href="/tags/breath-first-algorithm/">Breath First Algorithm</a> <a class="post-tag" href="/tags/path-finding/">Path Finding</a> <a class="post-tag" href="/tags/reinforcement-learning/">Reinforcement Learning</a> <a class="post-tag" href="/tags/rl/">RL</a> <a class="post-tag" href="/tags/a-algorithm/">A* Algorithm</a> <a class="post-tag" href="/tags/action-value-methods/">Action-value Methods</a> <a class="post-tag" href="/tags/dijkstra-s-algorithm/">Dijkstra's Algorithm</a> <a class="post-tag" href="/tags/visualization/">Visualization</a> <a class="post-tag" href="/tags/github/">Github</a> <a class="post-tag" href="/tags/markdown/">Markdown</a></div></div></div><div id="toc-wrapper" class="pl-0 pr-4 mb-5"> <span class="pl-3 pt-2 mb-2">Contents</span><nav id="toc" data-toggle="toc"></nav></div></div></div><div class="row"><div class="col-12 col-lg-11 col-xl-8"><div id="post-extend-wrapper" class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-md-4 pr-md-4"><div id="related-posts" class="mt-5 mb-2 mb-sm-4"><h3 class="pt-2 mt-1 mb-4 ml-1" data-toc-skip>Further Reading</h3><div class="card-deck mb-4"><div class="card"> <a href="/posts/RL-2-7/"><div class="card-body"> <span class="timeago small" > Jan 23, 2021 <i class="unloaded">2021-01-23T09:25:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Upper-Confidence-Bound Action Selection (UCB)</h3><div class="text-muted small"><p> Exploration in Reinforcement Learning 강화학습에서는 Exploration(탐험)이 굉장히 중요하다. Action value estimation에는 불확실성이 항상 존재하기 때문에 Exploration을 통해서 궁극적 학습 목표를 달성하고자 한다. ε-greedy method가 Exploration과 Exploita...</p></div></div></a></div><div class="card"> <a href="/posts/RL-2-2/"><div class="card-body"> <span class="timeago small" > Jan 2, 2021 <i class="unloaded">2021-01-02T09:25:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Action-value Methods</h3><div class="text-muted small"><p> Action-value Methods A k-armed Bandit Problem 에서 우리의 목적은 Value of the action (Expected total reward)를 최대화 하는 것이다. 목적을 달성하기 위해서 우리는 참 값인 Value of the action을 모르기 때문에 추정을 통해서 계산한다. 참 값인 Value of t...</p></div></div></a></div><div class="card"> <a href="/posts/RL-2-4/"><div class="card-body"> <span class="timeago small" > Jan 21, 2021 <i class="unloaded">2021-01-21T09:25:00+09:00</i> </span><h3 class="pt-0 mt-1 mb-3" data-toc-skip>Incremental Implementation</h3><div class="text-muted small"><p> Incremental Implementation이 뭔가? 행동의 가치(Action-value Methods)를 추정하는 방법에 대해서 우리는 관측된 보상의 표본 평균으로 가치를 추정했다. Incremental Implementation은 행동의 가치 추정치인 관측된 보상의 표본의 평균을 효율적으로 계산하는 방법이다. 행동의 가치 추정치 \(Q_...</p></div></div></a></div></div></div><div class="post-navigation d-flex justify-content-between"> <a href="/posts/dijkstra-algorithm/" class="btn btn-outline-primary"><p>Dijkstra's Algorithm</p></a> <a href="/posts/Astar-algorithm/" class="btn btn-outline-primary"><p>A* Algorithm</p></a></div><div id="disqus" class="pt-2 pb-2"><p class="font-italic text-center text-muted small"> Comments powered by <a href="https://disqus.com/">Disqus</a>.</p></div><script src="/assets/js/lib/jquery.disqusloader.min.js"></script> <script> var options = { scriptUrl: '//bekay.disqus.com/embed.js', disqusConfig: function() { this.page.url = 'https://bekaykang.github.io/posts/RL-2-1/'; this.page.identifier = '/posts/RL-2-1/'; } }; $.disqusLoader('#disqus', options); </script></div></div></div><script type="text/javascript" src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script> <script type="text/javascript"> const imgs = document.querySelectorAll('#post-wrapper img'); const observer = lozad(imgs); observer.observe(); </script><article> <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script></article><footer class="d-flex w-100 justify-content-center"><div class="d-flex justify-content-between align-items-center"><div class="footer-left"><p class="mb-0"> © 2023 <a href="https://twitter.com/username">Bekay</a>. <span data-toggle="tooltip" data-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author.">Some rights reserved.</span></p></div><div class="footer-right"><p class="mb-0"> Powered by <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a> with <a href="https://github.com/cotes2020/jekyll-theme-chirpy/" target="_blank" rel="noopener">Chirpy</a> theme.</p></div></div></footer></div><div id="search-result-wrapper" class="d-flex justify-content-center unloaded"><div class="col-12 col-xl-11 post-content"><div id="search-hints"><h4 class="text-muted mb-4">Trending Tags</h4><a class="post-tag" href="/tags/breath-first-algorithm/">Breath First Algorithm</a> <a class="post-tag" href="/tags/path-finding/">Path Finding</a> <a class="post-tag" href="/tags/reinforcement-learning/">Reinforcement Learning</a> <a class="post-tag" href="/tags/rl/">RL</a> <a class="post-tag" href="/tags/a-algorithm/">A* Algorithm</a> <a class="post-tag" href="/tags/action-value-methods/">Action value Methods</a> <a class="post-tag" href="/tags/dijkstra-s-algorithm/">Dijkstra's Algorithm</a> <a class="post-tag" href="/tags/visualization/">Visualization</a> <a class="post-tag" href="/tags/github/">Github</a> <a class="post-tag" href="/tags/markdown/">Markdown</a></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><div id="mask"></div><a id="back-to-top" href="#" aria-label="back-to-top" class="btn btn-lg btn-box-shadow" role="button"> <i class="fas fa-angle-up"></i> </a> <script src="https://cdn.jsdelivr.net/npm/simple-jekyll-search@1.7.3/dest/simple-jekyll-search.min.js"></script> <script> SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<div class="pl-1 pr-1 pl-sm-2 pr-sm-2 pl-lg-4 pr-lg-4 pl-xl-0 pr-xl-0"> <a href="https://bekaykang.github.io{url}">{title}</a><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"><div class="mr-sm-4"><i class="far fa-folder fa-fw"></i>{categories}</div><div><i class="fa fa-tag fa-fw"></i>{tags}</div></div><p>{snippet}</p></div>', noResultsText: '<p class="mt-5">Oops! No result founds.</p>' }); </script>
