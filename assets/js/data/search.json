[ { "title": "Install Python3.11 in WSL", "url": "/posts/python311_install_wsl/", "categories": "Issue Fix", "tags": "Python3.11, Install", "date": "2022-12-13 11:25:00 +0900", "snippet": "Python 3.11Python3.11 release되면서 많은 관심을 받고있다. 가장 큰 이유는 속도 측면에서 큰 개선이 있다. The Faster CPython Project is already yielding some exciting results. Python 3.11 is up to 10-60% faster than Python 3.10. On average, we measured a 1.22x speedup on the standard benchmark suite. See Faster CPython for deta..." }, { "title": "Force Directed Method", "url": "/posts/FDM/", "categories": "Optimization", "tags": "Visualization, Graph, Optimization", "date": "2022-11-02 21:25:00 +0900", "snippet": "Force Directed Method에 관한 게시물은 Universität Trier의 Philipp Kindermann 교수 강의를 바탕으로 작성하였습니다.Visualization of Graph데이터는 다양한 형태 및 특성을 가지고 있습니다. 특정 데이터셋은 Graph로 표현하는 것이 그 특성을 표현하는데 있어서 유리합니다. 예를 들어 나의 인간 관계를 데이터로 표현해야한다면 어떻게 표현하는 것이 가장 좋을까요? Graph 형태로 나의 인간 관계를 표현 하는것이 가장 직관적인 방법일 수 있습니다.어떻게 Graph를 가장 ..." }, { "title": "E169", "url": "/posts/E169/", "categories": "LeetCode, Data Structure", "tags": "Data Structure, Array, Easy", "date": "2022-06-20 21:25:00 +0900", "snippet": "169. Majority ElementSolutionclass Solution: def majorityElement(self, nums): num_list = sorted(set(nums)) for selec_num in num_list: if nums.count(selec_num) &amp;gt; len(nums)/2: return selec_numsolution = Solution()Examplenums = [2,2,1,1,1,2,2..." }, { "title": "KL Divergence", "url": "/posts/KL_divergence/", "categories": "Optimization", "tags": "KL Divergence, KL, Forward-KL, Reverse-KL", "date": "2022-03-25 21:25:00 +0900", "snippet": "KL Divergence는 무엇인가?Kullback-Leibler divergence(KL Divergence)는 하나의 확률분포로부터 다른 하나의 확률분포가 얼마나 다른지를 정량화한다.Bayesian theory에서 true distribution $P(X)$가 있을 때, 우리는 $P(X)$를 approximate distribution인 $Q(X)$로 추정하고자 한다. 이런 경우, 우리는 KL Divergence를 이용하여 approximate distribution $Q(X)$와 true distribution인 $P(X..." }, { "title": "Pytorch GPU Out of memory Issue", "url": "/posts/OutofMemory/", "categories": "Issue Fix", "tags": "Pytorch, GPU, Out of Memory", "date": "2021-08-12 21:25:00 +0900", "snippet": "GPU Out of Memory IssuePytorch를 이용하여 모델을 개발하면 아마 한번쯤은 “GPU Out of Memory”를 마주친다.GPU에 대한 이해가 깊다면, 금방 해결할 수 있지만 그렇지 않으면 당황하기 딱 좋은 Error다.나의 경험은 강화학습을 학습을 할 때, Policy Gradient 기반 Learner를 학습할 때 학습이 진행되면서 GPU Memory도 지속적으로 커지는 현상을 만났다.다양한 해결방법이 나와있지만, 나의 경우에 해결법은 torch.cuda.empty_cache() 이다!" }, { "title": "Diagrams - Diagram as Code", "url": "/posts/Diagrams/", "categories": "Visualization", "tags": "Visualization, Diagrams", "date": "2021-06-13 21:25:00 +0900", "snippet": "Diagrams: Diagram as CodePython Code만으로 Design tools 없이 전문가 처럼 깔끔한 Diagram을 그릴 수 있는 Lib이 있어서 소개한다.Diagrams기본적으로 AWS,Azure,Kubernetes,Alibaba Cloud,Oracle Cloud의 diagram을 제공하며, Custom도 가능하다.AWS에서 제공하는 Diagram의 정보는 아래 링크에서 확인이 가능하다.Diagram from AWSExample예시로, Neural Network에서 Inference의 과정을 Diagram..." }, { "title": "Policy Gradient", "url": "/posts/policy_gradient/", "categories": "Reinforcement Learning, Policy Gradient", "tags": "Policy Gradient, Reinforce", "date": "2021-05-16 11:25:00 +0900", "snippet": "Objective FunctionPolicy gradient의 objective는 결국 exptected return의 maximization!\\[\\begin{equation}J(\\theta_\\pi) = \\underset{\\tau \\sim \\pi_\\theta}{\\mathbb{E}} [R(\\tau)] \\tag{1} \\\\ \\\\\\end{equation}\\]만약 objective function $J(\\pi_\\theta)$의 derivation을 구할 수 잇다면 policy를 gradient ascent 방향으로 업데이트하면 expe..." }, { "title": "Markdown LaTex Math Symbols", "url": "/posts/markdown-latex/", "categories": "Github Tip", "tags": "Markdown, LaTex, Math, Symbols", "date": "2021-04-28 11:25:00 +0900", "snippet": "Markdown LaTex Symbols논문에서 나오는 수식들을 정리할때 자주 등장하는 symbols 위주로 정리!(내가 볼려고 하는 정리…ㅋㅋㅋ) Symbol Expression Symbol Expression \\(\\sum_{t=1}^{T}\\) $$\\sum_{t=1}^{T}$$ \\(\\int_{t=1}^{T}\\) $$\\int_{t=1}^{T}$$ \\(\\overset{T}{\\underset{t=0}{\\p..." }, { "title": "Python - super() 클래스 상속", "url": "/posts/Python-Super/", "categories": "Python", "tags": "Python, Class, super", "date": "2021-04-15 21:25:00 +0900", "snippet": "Python: super() 클래스 상속 :clipboard:Code를 작성하다보면 먼저 작성한 Class를 활용하면 좋을 때가 있다.예를 들면, 기존에 작성한 Class에서 구현한 함수를 지금 작성하는 Class에서 끌어다가 쓰고싶을 때이다.이럴떄 사용하는 방법이 클래스 상속 super()이다.예제를 하나 만들어보면,먼저 더하기, 빼기, 나누기, 곱하기의 기능을 구현할 수 있는 calculator()라는 정의했다.calculator Class의 기본 기능이 앞으로 작성할 다른 여러 Class에서 사용해야할 때, 작성하는 매 ..." }, { "title": "Upper-Confidence-Bound Action Selection (UCB)", "url": "/posts/RL-2-7/", "categories": "Reinforcement Learning, RL by Sutton & Barto", "tags": "Reinforcement Learning, RL, Action-value Methods, UCB, Upper-Confidence-Bound", "date": "2021-01-23 09:25:00 +0900", "snippet": "Exploration in Reinforcement Learning강화학습에서는 Exploration(탐험)이 굉장히 중요하다. Action value estimation에는 불확실성이 항상 존재하기 때문에 Exploration을 통해서 궁극적 학습 목표를 달성하고자 한다.ε-greedy method가 Exploration과 Exploitation을 강제적으로 ε의 확률로 balancing하는 것이다. ε-greedy method에서의 Exploration은 선택 가능한 Action 중에서 랜덤하게 하나의 Action을 선택한..." }, { "title": "Incremental Implementation", "url": "/posts/RL-2-4/", "categories": "Reinforcement Learning, RL by Sutton & Barto", "tags": "Reinforcement Learning, RL, Action-value Methods, Incremental Implementation", "date": "2021-01-21 09:25:00 +0900", "snippet": "Incremental Implementation이 뭔가?행동의 가치(Action-value Methods)를 추정하는 방법에 대해서 우리는 관측된 보상의 표본 평균으로 가치를 추정했다.Incremental Implementation은 행동의 가치 추정치인 관측된 보상의 표본의 평균을 효율적으로 계산하는 방법이다.행동의 가치 추정치 \\(Q_n\\)은 아래와 같다.\\[Q_n \\doteq {R_1 + R_2 + ... + R_n \\over n-1}\\]이 식을 이용하여 행동 가치를 추정한다면, 표본을 생성할 때 마다 매 보상을 메모리에..." }, { "title": "Github Markdown Emoji", "url": "/posts/markdown-emoji/", "categories": "Github Tip", "tags": "Github, Markdown, Emoji", "date": "2021-01-08 11:25:00 +0900", "snippet": "Markdown 작성할 때 다양한 Emoji Markup이 잘 정리되어 있어서 Markdown 작성시 참고하면 좋을 것 같다 (출처).People :bowtie: :bowtie: :smile: :smile: :laughing: :laughing: :blush: :blush: :smiley: :smiley: :relaxed: :relaxed: :smirk: :smirk: :heart_eyes: :heart_eyes: ..." }, { "title": "A* Algorithm for 3D Path Finding", "url": "/posts/Astar-3d-algorithm/", "categories": "Graph Search Algorithm", "tags": "Path Finding, Dijkstra's Algorithm, Breath First Algorithm, A* Algorithm", "date": "2021-01-07 11:25:00 +0900", "snippet": "A* Algorithm for 3D Path Finding지금까지 Graph Search Algorithm 중에서 Breath First Algorithm, Dijkstra’s Algorithm, 그리고 A* Algorithm에 대해서 알아봤다.이제는 알고리즘을 이용해서 조금 더 흥미로운 문제로 검증해보고자 한다.이번에는 3D Path Finding 문제를 A* Algorithm을 이용해서 잘 해결하는지 확인해보자.3D Path Finding3D Path Finding의 문제 정의는 아래와 같다. 30x30x30 크기의 Cu..." }, { "title": "Action-value Methods", "url": "/posts/RL-2-2/", "categories": "Reinforcement Learning, RL by Sutton & Barto", "tags": "Reinforcement Learning, RL, Action-value Methods, ε-greedy methods", "date": "2021-01-02 09:25:00 +0900", "snippet": "Action-value MethodsA k-armed Bandit Problem에서 우리의 목적은 Value of the action (Expected total reward)를 최대화 하는 것이다.목적을 달성하기 위해서 우리는 참 값인 Value of the action을 모르기 때문에 추정을 통해서 계산한다. 참 값인 Value of the action을 \\(q_*(a)\\)로 denote하고 추정값인 Value of the action을 \\(Q_t(a)\\)로 denote한다. 추정된 Value of the action값을 ..." }, { "title": "A* Algorithm", "url": "/posts/Astar-algorithm/", "categories": "Graph Search Algorithm", "tags": "Path Finding, Dijkstra's Algorithm, Breath First Algorithm, A* Algorithm", "date": "2021-01-01 11:25:00 +0900", "snippet": "A* AlgorithmPath Finding을 위한 Graph Search Algorithm 중에서 Breath First Algorithm와 Dijkstra’s Algorithm는 이전에 다뤘다. 이제 마지막인 A* Algorithm이다.Breath First Algorithm은 현재 위치에서 갈 수 있는 모든 방향을 다 탐색하면서 경로를 확장해나간다. 경로를 확장하다가 도착지점에 도착할 경우, 그 동안 지나온 경로를 거꾸로 밟아가며 Path를 찾는 방법이다.Dijkstra’s Algorithm은 Breah First Alg..." }, { "title": "A k-armed Bandit Problem", "url": "/posts/RL-2-1/", "categories": "Reinforcement Learning, RL by Sutton & Barto", "tags": "Reinforcement Learning, RL, Multi-armed Bandits", "date": "2020-12-31 09:25:00 +0900", "snippet": "A k-armed Bandit Problemk-armed bandit problem은 아래와 같다. Consider the following learning problem. You are faced repeatedly with a choice among k different options, or actions, After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the ac..." }, { "title": "Dijkstra&#39;s Algorithm", "url": "/posts/dijkstra-algorithm/", "categories": "Graph Search Algorithm", "tags": "Path Finding, Dijkstra's Algorithm, Breath First Algorithm, A* algirhtm", "date": "2020-12-28 11:25:00 +0900", "snippet": "Dijkstra’s AlgorithmGraph Searching Algorithm 중에서 Breath First Algorithm을 Cost 기반으로 알고리즘을 제안한것이 Dijkstra’s Algorithm이다.Cost라는 것은 아래 그림과 같이 에베레스트산과 같은 극한 지형이 있을 때 산을 가로 질러가는 것보다 산아래를 둘러가는 것이 효율적인 방법이다.이렇듯 길찾기를 할 때 위치마다 소모되는 체력?이 다를 수 있기 때문에 이러한 제약을 Cost로 적용하여 Path Finding하는 알고리즘이 Dijkstra’s Algori..." }, { "title": "Git Alias 설정으로 편하게 하기", "url": "/posts/git-alias/", "categories": "Github Tip", "tags": "Github, Git, Alias, shortcut", "date": "2020-12-19 09:25:00 +0900", "snippet": "Git Alias 설정으로 편하게 하기Git의 주요 command를 매번 수행할 때 마다 칠려면 성가신 일이다.기존의 특정 파일을 repository에 추가하는 것은 아래와 같이 3단계를 거친다. 터미널을 열고 현재의 Local Directory에서 변경사항을 적용한다 변경 사항을 repository로 commit이를 위해서 일반적으로 아래의 command를 따른다 $ git add . $ git commit -m &quot;message&quot; $ git push -u origin master ..." }, { "title": "Huber Loss &amp; F.smooth-l1-loss()", "url": "/posts/huber-loss/", "categories": "Optimization", "tags": "Optimization, Huber Loss, F.smooth-l1-loss", "date": "2020-12-09 09:25:00 +0900", "snippet": "Optimization &amp;amp; Loss Function일반적으로 최적화를 위해서는 최적화 대상을 설정해야한다.알고리즘에서는 참값과 예측값의 차이를 최적화 대상으로 선정하고 이를 Loss한다.일반적으로 L1 loss와 L2 loss가 가장 잘 알려져있다.L1 Loss는 아래와 같이 정의된다.\\[Loss(y,f(x))=\\sum_{i=1}^N |y_{i}-f(x_{i})|\\]L2 Loss는 아래와 같이 정의된다.\\[Loss(y,f(x))=\\sum_{i=1}^N (y_{i}-f(x_{i}))^2\\]두 방법 모두 장단점을 가..." }, { "title": "Github Blog - Google Analytics 연동", "url": "/posts/github-google-analytics/", "categories": "Github Tip", "tags": "Github Blog, Google Analytics, Tip", "date": "2020-12-01 21:25:00 +0900", "snippet": "GitHub Blog와 Google Analytics 연동Github Blog 자체적인 조회수에 대한 통계를 제공하지 않는다.Github Blog에 대한 통계를 도출하고 싶을 때 Google Analytics를 이용하면 가능하다.연동 방법먼저 Google Analytics에 가입을 해서 아래 그림의 순서대로 수행한다.4번 스트림 추가에서 “웹”을 선택하고 본인의 Github Blog 주소를 입력하면, 5번과 같이 데이터 스트림 항목이 생긴다.5번을 클릭하면 웹 스트림 세부정보가 나오게 되는데, 거기서 측정 ID를 복사한다.Gi..." }, { "title": "Python - Make a GIF", "url": "/posts/Python-Gif-Making/", "categories": "Visualization", "tags": "Visualization, GIF, Tip", "date": "2020-11-29 21:25:00 +0900", "snippet": "Python: GIF 만들기Python으로 결과를 시각화할 때 GIF를 생성하면 도움될 때가 많다.Python으로 연속적인 결과를 시각화할 때 GIF 만드는 코드를 간단하게 작성하였다.Code" }, { "title": "Breath First Algorithm", "url": "/posts/Breath-First-Algorithm/", "categories": "Graph Search Algorithm", "tags": "Path Finding, Breath First Algorithm, A* Algorithm", "date": "2020-11-29 12:30:00 +0900", "snippet": "Breath First AlgorithmGraph Searching Algorithm 중에서 Breath First Algorithm은 가장 단순한 형태의 알고리즘이다. Breath First Algorithm을 이용한 Path Finding 문제를 해결해보자.Code" } ]
